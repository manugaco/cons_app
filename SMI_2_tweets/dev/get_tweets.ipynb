{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 09:56:45.060455: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-03 09:56:45.060529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# LIBRARIES:\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB PARAMETERS\n",
    "\n",
    "schema = 'smi_schema'\n",
    "queries_path = '../build/queries/'\n",
    "\n",
    "with open('../../../../context/SMI/config/postgres.config') as config_file:\n",
    "    db_config = json.load(config_file)\n",
    "\n",
    "# Local database deployment\n",
    "conn = psycopg2.connect(\n",
    "                        dbname=db_config['db_name'],\n",
    "                        user=db_config['db_user'],\n",
    "                        host='localhost',\n",
    "                        port=db_config['db_port'],\n",
    "                        password=db_config['db_password'],\n",
    "                        options=db_config['db_options']\n",
    "                        )\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS:\n",
    "\n",
    "def fetchall_SQL(path, schema):\n",
    "    \"\"\"\n",
    "    Function to fetch all observations from a query to databasee:\n",
    "    params:\n",
    "        - path: relative path to the file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, 'r') as f: \n",
    "        query = f.read().format(schema=schema)\n",
    "\n",
    "    try:\n",
    "        #Execute query\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        db_fetch = cur.fetchall()\n",
    "        cur.close()\n",
    "        return(db_fetch)\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_datetweets_into_db(path, user_row, schema, conn):\n",
    "    '''\n",
    "    Function to update tweet dates on DB.\n",
    "    params:\n",
    "        - user_row: row to insert into the db.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        with open(path) as f:\n",
    "            cur.execute(\n",
    "                sql.SQL(f.read()).format(schema=sql.Identifier(schema)),\n",
    "                user_row\n",
    "            )\n",
    "\n",
    "            conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        conn.rollback()\n",
    "\n",
    "def df_to_postgres(df, table, conn):\n",
    "    \"\"\"\n",
    "    Function to save dataframe into postgres with copy_from:\n",
    "    params:\n",
    "        - conn: database connection.\n",
    "        - df: pandas dataframe.\n",
    "        - table: database table.\n",
    "    \"\"\"\n",
    "    #Buffering the dataframe into memory:\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, header=False, index=False)\n",
    "    buffer.seek(0)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        #Copy cached dataframe into postgres:\n",
    "        cur.copy_from(buffer, table, sep=\",\")\n",
    "        conn.commit()\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        conn.rollback()\n",
    "        cur.close()\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "def query_SQL(path, schema, conn):\n",
    "    \"\"\"\n",
    "    Function to make a query to database:\n",
    "    params:\n",
    "        - path: relative path to the file.\n",
    "    \"\"\"\n",
    "    # Read the SQL query from .sql file:\n",
    "    with open(path, 'r') as f:\n",
    "        query = f.read().format(schema=schema)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        conn.rollback()\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "def get_stops(stops, path):\n",
    "    \n",
    "    with open(path + stops[0]) as f:\n",
    "        stopw_1 = f.read().splitlines()\n",
    "    with open(path + stops[1]) as f:\n",
    "        stopw_2 = f.read().splitlines()\n",
    "\n",
    "    stopw_1[0] = stopw_1[0].replace('\\ufeff', '')\n",
    "    stopw = stopw_1 + stopw_2\n",
    "    stopw = [accent_rem(word) for word in stopw]\n",
    "    return(stopw)\n",
    "\n",
    "def get_ecofilter(path, files):\n",
    "    '''\n",
    "    Function to get the list of words to filter the tweets\n",
    "    '''\n",
    "    eco_filter = pd.DataFrame()\n",
    "    for file in files:\n",
    "        eco_filter = pd.concat([eco_filter, pd.read_excel(path + file, header=None)], axis=0)\n",
    "    eco_filter.drop_duplicates(inplace=True)\n",
    "    eco_filter = eco_filter.iloc[:,0].to_list()\n",
    "    #eco_filter = [lemmatize(word) for word in eco_filter]\n",
    "    return [accent_rem(word) for word in eco_filter]\n",
    "\n",
    "def accent_rem(name):\n",
    "    '''\n",
    "    Function to remove accents from an alphanumeric string:\n",
    "    params:\n",
    "        - name: character string.\n",
    "    Output: string without accents.\n",
    "    '''\n",
    "    #Define replacements (possible accents or other special char)\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ñ\", 'n'),\n",
    "        (\"à\", \"a\"),\n",
    "        (\"è\", \"e\"),\n",
    "        (\"ì\", \"i\"),\n",
    "        (\"ò\", \"o\"),\n",
    "        (\"ù\", \"u\"),\n",
    "        (\"ä\", 'a'),\n",
    "        (\"ë\", \"e\"),\n",
    "        (\"ï\", \"i\"),\n",
    "        (\"ö\", \"o\"),\n",
    "        (\"ü\", \"u\"),\n",
    "    )\n",
    "    #Replace with tuple:\n",
    "    for a, b in replacements:\n",
    "        name = name.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return(name)\n",
    "\n",
    "def tweet_cleaner(tweet, stopw, ecol):\n",
    "    '''\n",
    "    Function to treat the text of a tweet.\n",
    "    params:\n",
    "        - tweet: the document itself.\n",
    "    output: the tweet cleaned.\n",
    "    '''\n",
    "    # Remove urls (http in advance)\n",
    "    tweet = re.sub(r'http.*',\"\", tweet)\n",
    "    tweet = re.sub(r'pic.twitter\\S+', '', tweet)\n",
    "    # Remove mentions and hastags.\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    # Remove spanish vowel accents.\n",
    "    tweet = ' '.join([accent_rem(word) for word in tweet.split()])\n",
    "    # Remove special characters.\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "    # Lower captions.\n",
    "    tweet = tweet.lower()\n",
    "    # Remove numbers.\n",
    "    tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "    # Remove white spaces.\n",
    "    tweet = re.sub(' +', ' ', tweet)\n",
    "    # Remove stopwords.\n",
    "    tweet = ' '.join([word for word in tweet.split() if not word in stopw])\n",
    "    # Filter words length (< 1 and > 15).\n",
    "    tweet = ' '.join([word for word in tweet.split() if len(word) > 1 and len(word) <= 15])\n",
    "    # Filter ecolist.\n",
    "    commons = [word for word in ecol if word in tweet]\n",
    "    if len(commons) < 1:\n",
    "        tweet = ''\n",
    "    #if len(tweet) > 1:\n",
    "    #   #Lemmatize:\n",
    "    #    tweet = ' '.join([tok.lemma_.lower() for tok in nlp(tweet)])\n",
    "    return tweet\n",
    "\n",
    "def treat_text(df, text_col, stopw = [], ecol =[], date_col = 'date', sent_col = 'sentiment'):\n",
    "    '''\n",
    "    Function to treat text columns:\n",
    "    params:\n",
    "        - df: dataframe to treat.\n",
    "        - text_col: name of the text columns to treat.\n",
    "        - ecolist\n",
    "    Output: Dataframe treated.\n",
    "    '''\n",
    "    # Sanity checks :\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Formatting corpus columns:\n",
    "    if date_col == 'date':\n",
    "        print('Text mining job: Formatting the date column.')\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    if sent_col == 'sentiment':\n",
    "        print('Text mining job: Formatting the sentiment column.')\n",
    "        df[sent_col] = df[sent_col].replace(',','', regex=True)\n",
    "        df[sent_col] = df[sent_col].apply(lambda r: r.split('AGREEMENT')[0])\n",
    "        df[sent_col] = df[sent_col].apply(lambda r: r.split('DI')[0])\n",
    "\n",
    "    #Column text treatment:\n",
    "    print('Text mining job: Treat text column.')\n",
    "    df[text_col] = df[text_col].fillna(' ')\n",
    "    df[text_col] = df[text_col].apply(lambda r: tweet_cleaner(r, stopw, ecol))\n",
    "    df = df[df[text_col] != '']\n",
    "    if text_col == 'text':\n",
    "        df = df[['username', 'date', 'text']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return(df)\n",
    "\n",
    "def get_tweets(user, date_ini, date_end, stopw, ecolist):\n",
    "    '''\n",
    "    Function to get tweets from a user given a period range.\n",
    "    params:\n",
    "        - user: twitter user name.\n",
    "        - date_ini: first day of time window to retrieve tweets.\n",
    "        - date_end: last date of time window to retrieve tweets.\n",
    "    '''\n",
    "    # Tweets list:\n",
    "    twts_ls = []\n",
    "\n",
    "    # Twitter scrapper:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + user + ' since:' + date_ini + ' until:' + date_end).get_items()):\n",
    "        twts_ls.append([tweet.user.username, tweet.date, tweet.content])\n",
    "        \n",
    "    # Tweets dataframe: \n",
    "    df = pd.DataFrame(twts_ls, columns=['username', 'date', 'text'])\n",
    "    df = treat_text(df, 'text', stopw, ecolist, date_col = 'date', sent_col = None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERS\n",
    "\n",
    "path = '../../../../context/SMI/data/utils'\n",
    "stops = ['/db_stopwords_spanish_1.txt', '/db_stopwords_spanish_2.txt']\n",
    "stopw = get_stops(stops, path)\n",
    "ecofile = '/ecofilter.xlsx'\n",
    "ecofiles = [ecofile]\n",
    "eco_filter = get_ecofilter(path, ecofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database job: Select user randomly from DB\n",
      "Scrapping job: Retrieving tweets from user\n",
      "Text mining job: Formatting the date column.\n",
      "Text mining job: Treat text column.\n",
      "Scrapping job: Number of scrapped tweets: 0\n",
      "Database job: Inserting scrapped tweets on DB\n",
      "Database job: Scrapped tweets inserted on DB\n",
      "Database job: Removing duplicated entries\n",
      "Database job: Duplicated entries removed\n",
      "Database job: Inserting new scrapped date tweets into DB\n",
      "Database job: Scrapped date tweets inserted into DB\n"
     ]
    }
   ],
   "source": [
    "##get_tweets script body:\n",
    "\n",
    "#while True:\n",
    "\n",
    "## Select user randomly from date tweets table (SQL query) \"user\":\n",
    "print('Database job: Select user randomly from DB')\n",
    "user = fetchall_SQL(queries_path + 'SMI_get_random_user.sql', schema)[0]\n",
    "\n",
    "## Remove comma on the head of the vectorized version of the column \"smi_str_datetweets\" of the user and call it \"user_date_tweets\"\n",
    "## Create object with username called user_screename:\n",
    "user = list(user)\n",
    "if len(user[1]) > 0:\n",
    "    if user[1].split()[0] == ',':\n",
    "        user[1] = ', '.join(user[1].split(', ')[1:])\n",
    "\n",
    "user_screename = user[0]\n",
    "user_date_tweets = user[1]\n",
    "\n",
    "## Create vector of dates [2012-01-01, today] called \"all_dates\":\n",
    "all_dates = [str(date.date()) for date in pd.date_range('2012-01-01', pd.to_datetime(\"today\"), freq='D')]\n",
    "\n",
    "## Create vector of disjoint dates between \"all_dates\" and \"user_date_tweets\" and call it \"dates_to_scrap\":\n",
    "dates_to_scrap = list(set(all_dates) - set(user_date_tweets))\n",
    "\n",
    "## Select randomly one of \"dates_to_scrap\":\n",
    "ini_date = random.choice(dates_to_scrap)\n",
    "\n",
    "## Create end_date as initial_date + 1 day:\n",
    "end_date = (pd.to_datetime(ini_date) + pd.DateOffset(days=1)).date().strftime('%Y-%m-%d')\n",
    "\n",
    "## Scrap the tweets of the user on that dates range:\n",
    "print('Scrapping job: Retrieving tweets from user')\n",
    "df_tweets = get_tweets(user_screename, ini_date, end_date, stopw, eco_filter)\n",
    "print('Scrapping job: Number of scrapped tweets: ' + str(df_tweets.shape[0]))\n",
    "\n",
    "## Insert the tweets in the tweets table (df_to_postgres) if there are tweets, otherwise do nothing\n",
    "print('Database job: Inserting scrapped tweets on DB')\n",
    "df_to_postgres(df_tweets, 'smi_tweets', conn)\n",
    "print('Database job: Scrapped tweets inserted on DB')\n",
    "\n",
    "## Execute SQL query to remove duplicated entries on the tweets table:\n",
    "print('Database job: Removing duplicated entries')\n",
    "query_SQL(queries_path + 'SMI_remove_dup_tweets.sql', schema, conn)\n",
    "print('Database job: Duplicated entries removed')\n",
    "\n",
    "## Update new retrieval date on smi_date_tweets table on DB\n",
    "print('Database job: Inserting new scrapped date tweets into DB')\n",
    "user_to_insert = user.copy()\n",
    "user_to_insert[1] = ', '.join(user_to_insert[1].split(', ') + [ini_date])\n",
    "insert_datetweets_into_db(queries_path + 'SMI_insert_date_tweets.sql', tuple(user_to_insert), schema, conn)\n",
    "print('Database job: Scrapped date tweets inserted into DB')\n",
    "\n",
    "## SLEEP n seconds, choose n randomly in the interval [60, 120]\n",
    "time.sleep(random.randint(60, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeteOrihuela</td>\n",
       "      <td>2016-02-29 14:27:10+00:00</td>\n",
       "      <td>interior sureste peninsular inmediaciones imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MeteOrihuela</td>\n",
       "      <td>2016-02-29 10:04:55+00:00</td>\n",
       "      <td>estabilidad atmosferica amanecer imagen andres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                      date  \\\n",
       "0  MeteOrihuela 2016-02-29 14:27:10+00:00   \n",
       "1  MeteOrihuela 2016-02-29 10:04:55+00:00   \n",
       "\n",
       "                                                text  \n",
       "0  interior sureste peninsular inmediaciones imag...  \n",
       "1  estabilidad atmosferica amanecer imagen andres...  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
