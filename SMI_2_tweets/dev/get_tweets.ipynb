{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 16:27:42.564355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-24 16:27:42.564439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries and packages\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accent_rem(name):\n",
    "    '''\n",
    "    Function to remove accents from an alphanumeric string:\n",
    "    params:\n",
    "        - name: character string.\n",
    "    Output: string without accents.\n",
    "    '''\n",
    "    #Define replacements (possible accents or other special char)\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ñ\", 'n'),\n",
    "        (\"à\", \"a\"),\n",
    "        (\"è\", \"e\"),\n",
    "        (\"ì\", \"i\"),\n",
    "        (\"ò\", \"o\"),\n",
    "        (\"ù\", \"u\"),\n",
    "        (\"ä\", 'a'),\n",
    "        (\"ë\", \"e\"),\n",
    "        (\"ï\", \"i\"),\n",
    "        (\"ö\", \"o\"),\n",
    "        (\"ü\", \"u\"),\n",
    "    )\n",
    "    #Replace with tuple:\n",
    "    for a, b in replacements:\n",
    "        name = name.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return(name)\n",
    "\n",
    "def get_stops(stops, path):\n",
    "    \n",
    "    with open(path + stops[0]) as f:\n",
    "        stopw_1 = f.read().splitlines()\n",
    "    with open(path + stops[1]) as f:\n",
    "        stopw_2 = f.read().splitlines()\n",
    "\n",
    "    stopw_1[0] = stopw_1[0].replace('\\ufeff', '')\n",
    "    stopw = stopw_1 + stopw_2\n",
    "    stopw = [accent_rem(word) for word in stopw]\n",
    "    return(stopw)\n",
    "\n",
    "def get_ecofilter(path, files):\n",
    "    '''\n",
    "    Function to get the list of words to filter the tweets\n",
    "    '''\n",
    "    eco_filter = pd.DataFrame()\n",
    "    for file in files:\n",
    "        eco_filter = pd.concat([eco_filter, pd.read_excel(path + file, header=None)], axis=0)\n",
    "    eco_filter.drop_duplicates(inplace=True)\n",
    "    eco_filter = eco_filter.iloc[:,0].to_list()\n",
    "    return [lemmatize(accent_rem(word)).lower() for word in eco_filter]\n",
    "\n",
    "def remove_stops(tweet, stopw):\n",
    "    return ' '.join([word for word in tweet.split() if not word in stopw])\n",
    "\n",
    "def filter_noneco(tweet, ecolist):\n",
    "    '''\n",
    "    Function to check whether the tweet has economic topic or not:\n",
    "    params:\n",
    "        - tweet: the document itself.\n",
    "        - ecolist: list of words related to economy and politics.\n",
    "    output: the tweets if it contains at least one word in the ecolist.\n",
    "    '''\n",
    "    commons = list(set(ecolist).intersection(tweet))\n",
    "    if len(commons) <= 1:\n",
    "        tweet = ''\n",
    "    return tweet\n",
    "\n",
    "def get_ecotweets(df, ecolist, text_col = 'text'):\n",
    "    \n",
    "    df[text_col] = df[text_col].apply(lambda r: filter_noneco(r, ecolist))\n",
    "    df = df[df[text_col] != '']\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    doc = nlp(tweet)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def trail_ws(tweet):\n",
    "    return re.sub(' +', ' ', tweet)\n",
    "\n",
    "def remove_num(tweet):\n",
    "    return ''.join([i for i in tweet if not i.isdigit()])\n",
    "\n",
    "def treat_text(df, text_col, stopw = [], ecolist = [], date_col = 'date', sent_col = 'sentiment'):\n",
    "        '''\n",
    "        Function to treat the corpus columns:\n",
    "        params:\n",
    "            - df: dataframe to treat.\n",
    "        Output: Dataframe treated.\n",
    "        '''\n",
    "        # Sanity checks:\n",
    "        df = df.fillna('')\n",
    "        \n",
    "        # Formatting corpus columns:\n",
    "\n",
    "        if date_col == 'date':\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "        if sent_col == 'sentiment':\n",
    "            df[sent_col] = df[sent_col].replace(',', '', regex=True)\n",
    "\n",
    "        #Columns treatment:\n",
    "        df[text_col] = df[text_col].apply(lambda r: ' '.join([accent_rem(name) for name in r.split()]))\n",
    "        df[text_col] = df[text_col].apply(lambda r: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", r).split()))\n",
    "        df[text_col] = df[text_col].apply(lambda r: r.lower())\n",
    "        df[text_col] = df[text_col].apply(lambda r: remove_num(r))\n",
    "        df[text_col] = df[text_col].apply(lambda r: trail_ws(r))\n",
    "        df[text_col] = df[text_col].apply(lambda r: remove_stops(r, stopw))\n",
    "        df[text_col] = df[text_col].apply(lambda r: lemmatize(r))\n",
    "        df[text_col] = df[text_col].apply(lambda r: ' '.join([accent_rem(name) for name in r.split()]))\n",
    "    \n",
    "        # Filter:\n",
    "        df = df[df[text_col] != '']\n",
    "\n",
    "        return(df.reset_index(drop=True))\n",
    "\n",
    "def get_tweets(user, date_ini, date_end, stopw, ecolist):\n",
    "    '''\n",
    "    Function to get tweets from a user given a period range.\n",
    "    params:\n",
    "        - user: twitter user name.\n",
    "        - date_ini: first day of time window to retrieve tweets.\n",
    "        - date_end: last date of time window to retrieve tweets.\n",
    "    '''\n",
    "    # Tweets list:\n",
    "    twts_ls = []\n",
    "\n",
    "    # Twitter scrapper:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + user + ' since:' + date_ini + ' until:' + date_end).get_items()):\n",
    "        twts_ls.append([tweet.user.username, tweet.date, tweet.content])\n",
    "        \n",
    "    # Tweets dataframe: \n",
    "    df = pd.DataFrame(twts_ls, columns=['username', 'date', 'text'])\n",
    "    df = treat_text(df, 'text', stopw, ecolist, date_col = 'date', sent_col = None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../../context/SMI/data/utils'\n",
    "stops = ['/db_stopwords_spanish_1.txt', '/db_stopwords_spanish_2.txt']\n",
    "stopw = get_stops(stops, path)\n",
    "ecofile = '/ecofilter.xlsx'\n",
    "ecofiles = [ecofile]\n",
    "eco_filter = get_ecofilter(path, ecofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Add stop words to database\n",
    "# Add ecolist to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "\n",
    "user = '08sesion'\n",
    "date_ini = '2015-10-21'\n",
    "date_end = '2015-10-23'\n",
    "\n",
    "df_tweets = get_tweets(user, date_ini, date_end, stopw, eco_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [username, date, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ecotweets(df_tweets, eco_filter, 'text')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
