{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db config\n",
    "with open('../../../../context/SMI/config/postgres.config') as config_file:\n",
    "    db_config = json.load(config_file)\n",
    "\n",
    "# Local database deployment\n",
    "conn = psycopg2.connect(\n",
    "                        dbname=db_config['db_name'],\n",
    "                        user=db_config['db_user'],\n",
    "                        host='localhost',\n",
    "                        port=db_config['db_port'],\n",
    "                        password=db_config['db_password'],\n",
    "                        options=db_config['db_options']\n",
    "                        )\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "schema = db_config['db_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories:\n",
    "if not os.path.isdir(logs_path):\n",
    "    print('Environment job: Creating ' + app_name + ' logs directory')\n",
    "    os.makedirs(logs_path)\n",
    "\n",
    "if not os.path.isdir(temp_data_path):\n",
    "    print('Environment job: Creating ' + app_name + ' data directory')\n",
    "    os.makedirs(temp_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment job: Checking folder logs/\n",
      "Environment job: Folder logs/ exists\n",
      "Environment job: Checking folder ../data/\n",
      "Environment job: Folder ../data/ exists\n"
     ]
    }
   ],
   "source": [
    "# Directories check:\n",
    "print('Environment job: Checking folder '+ logs_path)\n",
    "if os.path.isdir(logs_path):\n",
    "    print('Environment job: Folder '+ logs_path + ' exists')\n",
    "else:\n",
    "    print('Environment job: Folder '+ logs_path + ' does not exists')\n",
    "    print('Environment job: Creating folder '+ logs_path)\n",
    "    os.makedirs(logs_path)\n",
    "\n",
    "print('Environment job: Checking folder '+ temp_data_path)\n",
    "if os.path.isdir(temp_data_path):\n",
    "    print('Environment job: Folder '+ temp_data_path + ' exists')\n",
    "else:\n",
    "    print('Environment job: Folder '+ temp_data_path + ' does not exists')\n",
    "    print('Environment job: Creating folder '+ temp_data_path)\n",
    "    os.makedirs(temp_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseCreation:\n",
    "    '''\n",
    "    Database creation and initial data insertion:\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                queries_path,\n",
    "                conn,\n",
    "                schema\n",
    "                ):\n",
    "        self.queries_path = queries_path\n",
    "        self.conn = conn\n",
    "        self.cur = cur\n",
    "        self.schema = schema\n",
    "\n",
    "    @staticmethod\n",
    "    def get_info(url, header):\n",
    "        '''\n",
    "        Function to get the initial twitter users file.\n",
    "        Params: \n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Uncleaned dataframe with parsed tables.\n",
    "        '''\n",
    "        try:\n",
    "\n",
    "            #Get text from url:\n",
    "            page = requests.get(url, headers=header)\n",
    "            soup = BeautifulSoup(page.text, \"lxml\")\n",
    "            results = soup.find(id=\"listado\")\n",
    "\n",
    "            #Get table from text:\n",
    "            df = pd.DataFrame([[tr for tr in tab] for tab in results.table])\n",
    "            df = df.loc[:,1:]\n",
    "\n",
    "            #Get column names:\n",
    "            colnames = [str(name) for name in df.iloc[0]]\n",
    "            colnames = [re.search('<b>(.*)</b>', name).group(1).replace('<br/>', ' ') for name in colnames if name != 'None']\n",
    "            df = df.loc[1:,2:]\n",
    "            df.columns = colnames\n",
    "\n",
    "            #Dropping columns:\n",
    "            df = df[~df['Twittero'].isnull()].reset_index(drop=True)\n",
    "            return(df)\n",
    "\n",
    "        except:\n",
    "            print('Get info from XML files error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_user(input):\n",
    "        '''\n",
    "        Function to extract user name from the list.\n",
    "        Params:\n",
    "            - input: Twitter message.\n",
    "        Output: Twitter user.\n",
    "        '''\n",
    "        try:\n",
    "            #Locating the username:\n",
    "            s = str(input)\n",
    "            start = s.find(\">@\") + len(\">@\")\n",
    "            end = s.find(\"<br/\")\n",
    "            substring = s[start:end]\n",
    "\n",
    "            #Returning the username from:\n",
    "            return(substring)\n",
    "\n",
    "        except:\n",
    "            print('Get username from XML format error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n(input):\n",
    "        '''\n",
    "        Function to extract numeric values from the table:\n",
    "        Params:\n",
    "            - input: Different values from twitter users accounts.\n",
    "        Output: Value\n",
    "        '''\n",
    "        try:\n",
    "            #Locating the value;\n",
    "            s = str(input)\n",
    "            start = s.find(\">\") + len(\">\")\n",
    "            end = s.find(\"</td\")\n",
    "            substring = s[start:end]\n",
    "\n",
    "            #Returning the value:\n",
    "            return(substring.replace(',', ''))\n",
    "\n",
    "        except:\n",
    "            print('Get numeric variables from XML format error')\n",
    "            return 1\n",
    "\n",
    "    def get_initial_users_table(self, urls, headers):\n",
    "        '''\n",
    "        Function to create the initial users dataframe.\n",
    "        Params:\n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Dataframe with cleaned data.\n",
    "        '''\n",
    "        try:\n",
    "            #Scrap info from wp:\n",
    "            info = self.get_info(urls, headers)\n",
    "\n",
    "            #Columns to clean:\n",
    "            cols = ['Twittero', 'Seguido por', 'Sigue a', 'Tweets', 'Twitea desde', 'Ultimo Tweet', 'Categoria']\n",
    "\n",
    "            #Clean columns:\n",
    "            for col in cols:\n",
    "                if col == 'Twittero':\n",
    "                    users = [self.get_user(info[col][i]) for i in range(info.shape[0])]\n",
    "                    info[col] = users\n",
    "                else:\n",
    "                    info[col] = [self.get_n(info[col][i]) for i in range(info.shape[0])]\n",
    "            return(info)\n",
    "\n",
    "        except:\n",
    "            print('Get initial users table')\n",
    "            return 1\n",
    "\n",
    "    def get_tw_users_list(self, urls, headers, ini_users_dict):\n",
    "        '''\n",
    "        Function to get all users from different urls\n",
    "        Params:\n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Users list and users table\n",
    "        '''\n",
    "        #Get all users from tables of different sections:\n",
    "        try:\n",
    "            users = []\n",
    "            df_out = pd.DataFrame()\n",
    "            for i in range(len(urls)):\n",
    "                \n",
    "                #Create users dataframe and users list:\n",
    "                df = self.get_initial_users_table(urls[i][0], headers)\n",
    "                df_out = pd.concat([df_out, df], axis=0).reset_index(drop=True)\n",
    "                users.extend(df['Twittero'].to_list())\n",
    "\n",
    "            #Drop duplicates:\n",
    "            if len(users) != len(set(users)):\n",
    "                users = list(set(users))\n",
    "                df_out = df_out.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "            #Formating columns:\n",
    "            df_out.columns = [key for key in list(ini_users_dict.keys())]\n",
    "            df_out = df_out.astype(ini_users_dict)\n",
    "            df_out['lastTweet'] = np.where(df_out['lastTweet']=='n/d', df_out['tweetsSince'], df_out['lastTweet'])\n",
    "            df_out['tweetsSince']=pd.to_datetime(df_out['tweetsSince'])\n",
    "            df_out['lastTweet']=pd.to_datetime(df_out['lastTweet'])\n",
    "            df_out = df_out.fillna(0)\n",
    "            return(df_out)\n",
    "\n",
    "        except:\n",
    "            print('Get twitter users list error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_usrs_loc(df, munlist):\n",
    "        '''\n",
    "        Function to filter the location field given a municipalities list, to ensure spanish users:\n",
    "        params:\n",
    "            - df: input dataframe with users information:\n",
    "            - munlist: list of municipalities:\n",
    "        Output: filtered users table.\n",
    "        '''\n",
    "        #Convert location field to lower case:\n",
    "        df['location'] = df['location'].apply(lambda r: r.replace(',', ''))\n",
    "        df['location'] = df['location'].apply(lambda r: r.lower())\n",
    "\n",
    "        #Filter location:\n",
    "        df = df[df['location'].isin(munlist)]\n",
    "        \n",
    "        return(df)\n",
    "\n",
    "    def backup_check(self, path, db_munlist, kind):\n",
    "        '''\n",
    "        Function to check whether there are backups.\n",
    "        params:\n",
    "            - path: relative path to the backup file.\n",
    "            - kind: initial users or users.\n",
    "        Output: \n",
    "            - df: users dataframe.\n",
    "            - usr_ls: list of screenName users.\n",
    "            - check: boolean to check whether there are backup or not.\n",
    "        '''\n",
    "        print('Data job: Check if ' + kind + ' backup exists.')\n",
    "        if os.path.isfile(path):\n",
    "\n",
    "            print('Data job: ' + kind + ' backup exists. Loading file: ' + path)\n",
    "            with open(path, 'r') as f:\n",
    "\n",
    "                df = pd.json_normalize(json.load(f))\n",
    "                if kind == 'users':\n",
    "                    print('Data Engineering job: Filtering location from backup users.')\n",
    "                    print('Data Engineering job: Observations before filter: ' + str(df.shape[0]))\n",
    "                    df = self.filter_usrs_loc(df, db_munlist)\n",
    "                    df['ff_lookup'] = False\n",
    "                    print('Data Engineering job: Observations after filter: ' + str(df.shape[0]))\n",
    "                usr_ls = df['screenName'].to_list()\n",
    "                check = True\n",
    "                print('Data job: ' + kind + ' backup from json file retrieved.')\n",
    "        else:\n",
    "\n",
    "            print('Data job: ' + kind + ' backup does not exists. ')\n",
    "            df = pd.DataFrame()\n",
    "            usr_ls = []\n",
    "            check = False\n",
    "        \n",
    "        return(df, usr_ls, check)\n",
    "\n",
    "    @staticmethod\n",
    "    def fetchone_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to fetch one observation from a query to database:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                return(cur.fetchone()[0])\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error) \n",
    "    \n",
    "    @staticmethod\n",
    "    def fetchall_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to fetch all observations from a query to databasee:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f: \n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                db_fetch = cur.fetchall()\n",
    "                db_fetch = [db_fetch[i][0] for i in range(len(db_fetch))]\n",
    "                return(db_fetch)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "\n",
    "    @staticmethod\n",
    "    def query_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to make a query to database:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_postgres(conn, df, table):\n",
    "        \"\"\"\n",
    "        Function to save dataframe into postgres with copy_from:\n",
    "        params:\n",
    "            - conn: database connection.\n",
    "            - df: pandas dataframe.\n",
    "            - table: database table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #Buffering the dataframe into memory:\n",
    "            buffer = StringIO()\n",
    "            df.to_csv(buffer, header=False, index=False)\n",
    "            buffer.seek(0)\n",
    "\n",
    "            #Copy cached dataframe into postgres:\n",
    "            cur = conn.cursor()\n",
    "            cur.copy_from(buffer, table, sep=\",\")\n",
    "            conn.commit()\n",
    "            \n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            conn.rollback()\n",
    "            cur.close()\n",
    "            print(error)\n",
    "            return 1\n",
    "        cur.close()\n",
    "\n",
    "    def db_cs(self):\n",
    "        '''\n",
    "        Function to check and create the database schema and tables.\n",
    "        params: selft referenced, no params.\n",
    "        '''\n",
    "        # Check if the schema exists.\n",
    "        print('Database job: Check if SMI schema exists.')\n",
    "        schema_check = self.fetchone_SQL(queries_path + 'SMI_schema_check.sql')\n",
    "\n",
    "        # If schema exist, check tables.\n",
    "        if schema_check:\n",
    "\n",
    "            # Check initial users table.\n",
    "            print('Database job: SMI schema exists on DB.')\n",
    "            print('Database job: Check if initial users table exist on DB.')\n",
    "            db_ini_check = self.fetchone_SQL(queries_path + 'SMI_ini_users_check.sql')\n",
    "\n",
    "            # If exists, do nothing.\n",
    "            if db_ini_check:\n",
    "                print('Database job: Initial users table exist on DB.')\n",
    "\n",
    "            # If it does not exist, create initial users table.\n",
    "            else:\n",
    "                print('Database job: Initial users table does not exist on DB.')\n",
    "                print('Database job: Creating initial users table on DB.')\n",
    "                self.query_SQL(queries_path + 'SMI_ini_users_table_creation.sql')\n",
    "                print('Database job: Initial users table created on DB.')\n",
    "\n",
    "            # Check users table.\n",
    "            print('Database job: Check if users table exist on DB.')\n",
    "            db_usr_check = self.fetchone_SQL(queries_path + 'SMI_usrs_table_check.sql')\n",
    "\n",
    "            #If exists, do nothing.\n",
    "            if db_usr_check:\n",
    "                print('Database job: Users table exist on DB.')\n",
    "\n",
    "            #If it does not exist, create users table.\n",
    "            else:\n",
    "                print('Database job: Users table does not exist on DB.')\n",
    "                print('Database job: Creating users table on DB.')\n",
    "                self.query_SQL(queries_path + 'SMI_usrs_table_creation.sql')\n",
    "                print('Database job: Users table created on DB.')\n",
    "\n",
    "        # If schema does not exists, create schema and tables.\n",
    "        else:\n",
    "            print('Database job: SMI schema does not exist.')\n",
    "            print('Database job: Cold start - Creating SMI schema and tables on DB.')\n",
    "            self.query_SQL(queries_path + 'SMI_coldstart_database.sql')\n",
    "            print('Database job: DB schema and tables created.')\n",
    "\n",
    "    def scrap_users(self):\n",
    "        '''\n",
    "        Function to scrap initial users from url:\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        # Scrap ini users, create backup and fill database table:\n",
    "        print('Scraping job: Retrieve initial users from url.')\n",
    "        df = self.get_tw_users_list(urls, headers, ini_users_dict)\n",
    "        print('Scraping job: Initial users from url retrieved.')\n",
    "        print('Scraping job: Save initial users to json backup.')\n",
    "        df.to_json(temp_data_path + 'db_ini_users_' + db_today + '.json', orient='records', date_format='iso')\n",
    "        self.df_to_postgres(conn, df, initial_users_table)\n",
    "        print('Database job: Initial users table created on DB.')\n",
    "\n",
    "    def insert_ini_users(self):\n",
    "        '''\n",
    "        Function to insert initial users into DB.\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        db_ini_ls = self.fetchall_SQL(queries_path + 'SMI_ini_database_screenName.sql')\n",
    "        path_ini = temp_data_path + 'db_ini_users_' + db_ini_users_bkp + '.json'\n",
    "        df_ini_users, df_ini_ls, df_ini_user_check = self.backup_check(path_ini, db_munlist, kind = 'initial users')\n",
    "\n",
    "        if df_ini_user_check:\n",
    "            \n",
    "            if (len(db_ini_ls) == 0):\n",
    "                print('Database job: Initial users table is empty.')\n",
    "                print('Database job: Insert initial users backup into DB.')\n",
    "                self.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "            else:\n",
    "                print('Database job: Initial users table is not empty.')\n",
    "                print('Data job: Compare initial users on DB and backup.')\n",
    "\n",
    "                df_ini_ls.sort()\n",
    "                db_ini_ls.sort()\n",
    "                \n",
    "                if df_ini_ls == db_ini_ls:\n",
    "                    print('Data job: initial users match.')\n",
    "                else:\n",
    "                    print('Database job: initial users do not match, drop and create initial users table.')\n",
    "                    #Create initial users table:\n",
    "                    print('Database job: Creating initial users table on DB.')\n",
    "                    self.query_SQL(queries_path + 'SMI_ini_users_table_creation.sql')\n",
    "                    \n",
    "                    #Initial users table insertion:\n",
    "                    print('Database job: Insert initial users back into DB.')\n",
    "                    self.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "                    print('Database job: Initial users table inserted on DB.')\n",
    "        else:\n",
    "            self.scrap_users()\n",
    "\n",
    "    def insert_users(self):\n",
    "        '''\n",
    "        Function to insert users backup into DB.\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        db_usr_ls = self.fetchall_SQL(queries_path + 'SMI_usrs_database_screenName.sql')\n",
    "        path_usr = temp_data_path + 'db_users_' + db_users_bkp + '.json'\n",
    "        df_usr, df_usr_ls, df_usr_check = self.backup_check(path_usr, db_munlist, kind = 'users')\n",
    "\n",
    "        if df_usr_check:\n",
    "            \n",
    "            if (len(db_usr_ls) == 0):\n",
    "                print('Database job: Users table is empty.')\n",
    "                print('Database job: Insert users backup into DB.')\n",
    "                self.df_to_postgres(conn, df_usr, users_table)\n",
    "            else:\n",
    "                print('Database job: Users table is not empty.')\n",
    "                print('Data job: Compare users on db and backup.')\n",
    "\n",
    "                df_usr_ls.sort()\n",
    "                db_usr_ls.sort()\n",
    "                \n",
    "                if df_usr_ls == db_usr_ls:\n",
    "                    print('Data job: Users match.')\n",
    "                else:\n",
    "                    print('Database job: Users do not match, drop and create users table.')\n",
    "                    #Create initial users table:\n",
    "                    print('Database job: Creating users table on DB.')\n",
    "                    self.query_SQL(queries_path + 'SMI_usrs_table_creation.sql')\n",
    "                    \n",
    "                    #Initial users table insertion:\n",
    "                    print('Database job: Users back into DB.')\n",
    "                    self.df_to_postgres(conn, df_usr, users_table)\n",
    "                    print('Database job: Users table inserted on DB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database job: Check if SMI schema exists.\n",
      "Database job: SMI schema exists on DB.\n",
      "Database job: Check if initial users table exist on DB.\n",
      "Database job: Initial users table exist on DB.\n",
      "Database job: Check if users table exist on DB.\n",
      "Database job: Users table exist on DB.\n",
      "Data job: Check if initial users backup exists.\n",
      "Data job: initial users backup exists. Loading file: ../data/db_ini_users_2022-02-08.json\n",
      "Data job: initial users backup from json file retrieved.\n",
      "Database job: Initial users table is not empty.\n",
      "Data job: Compare initial users on DB and backup.\n",
      "Data job: initial users match.\n",
      "Data job: Check if users backup exists.\n",
      "Data job: users backup exists. Loading file: ../data/db_users_2018-06-01.json\n",
      "Data Engineering job: Filtering location from backup users.\n",
      "Data Engineering job: Observations before filter: 673327\n",
      "Data Engineering job: Observations after filter: 240232\n",
      "Data job: users backup from json file retrieved.\n",
      "Database job: Users table is empty.\n",
      "Database job: Insert users backup into DB.\n"
     ]
    }
   ],
   "source": [
    "#Create class instance:\n",
    "dbcreate = DatabaseCreation(queries_path, conn, schema)\n",
    "\n",
    "## Check schema and tables:\n",
    "dbcreate.db_cs()\n",
    "\n",
    "## Check backups, tables and fill database:\n",
    "## Initial users:\n",
    "dbcreate.insert_ini_users()\n",
    "\n",
    "## Users:\n",
    "dbcreate.insert_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = '../../../../context/SMI/data/train_model/TASScorpus.json'\n",
    "with open(path_corpus, 'r') as f:\n",
    "    df = pd.json_normalize(json.load(f))\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df['content'] = df['content'].replace('\"','', regex=True)\n",
    "    df['content'] = df['content'].replace(',','', regex=True)\n",
    "    df['content'] = df['content'].replace(r'\\\\',' ', regex=True)\n",
    "    df['sentiment'] = df['sentiment'].replace(',','', regex=True)\n",
    "    df['content'] = df['content'].replace(r'\\r+|\\n+|\\t+','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid              object\n",
       "user                 object\n",
       "content              object\n",
       "date         datetime64[ns]\n",
       "lang                 object\n",
       "sentiment            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perfectamente imperfecto. Besos RT @CiindyRomero: @AlejandroSanz no lo veo pero lo comienzo a sentir. Será perfecto! 355 240 275 355 262 277'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['tweetid'] == '172131381843984385'].iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = '../../../../context/SMI/data/municipalities/munlist.json'\n",
    "with open(path_corpus, 'r') as f:\n",
    "    df = pd.DataFrame(json.load(f), columns=['location'])\n",
    "    df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a arnoia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a baña</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a capela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a cañiza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>álava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>la tierra llana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>cruz de tenerife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>de tenerife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11700</th>\n",
       "      <td>de navarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11701 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               location\n",
       "0              a arnoia\n",
       "1                a baña\n",
       "2                a bola\n",
       "3              a capela\n",
       "4              a cañiza\n",
       "...                 ...\n",
       "11696             álava\n",
       "11697   la tierra llana\n",
       "11698  cruz de tenerife\n",
       "11699       de tenerife\n",
       "11700        de navarra\n",
       "\n",
       "[11701 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_postgres(df, table, conn):\n",
    "    \"\"\"\n",
    "    Function to save dataframe into postgres with copy_from:\n",
    "    params:\n",
    "        - conn: database connection.\n",
    "        - df: pandas dataframe.\n",
    "        - table: database table.\n",
    "    \"\"\"\n",
    "    #Buffering the dataframe into memory:\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, header=False, index=False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    #Copy cached dataframe into postgres:\n",
    "    cur = conn.cursor()\n",
    "    cur.copy_from(buffer, table, sep=\",\")\n",
    "    conn.commit()\n",
    "\n",
    "def treat_text(df, text_col, date_col = 'date', sent_col = 'sentiment'):\n",
    "    '''\n",
    "    Function to treat the corpus columns:\n",
    "    params:\n",
    "        - df: dataframe to treat.\n",
    "    Output: Dataframe treated.\n",
    "    '''\n",
    "\n",
    "    #Columns treatment:\n",
    "    \n",
    "    df[text_col] = df[text_col].replace(',','', regex=True)\n",
    "    df[text_col] = df[text_col].replace('\"','', regex=True)\n",
    "    df[text_col] = df[text_col].replace(r'\\\\',' ', regex=True)\n",
    "    df[text_col] = df[text_col].replace(r'\\r+|\\n+|\\t+','', regex=True)\n",
    "    df[text_col]= df[text_col].replace('[^a-zA-Z0-9]', '')\n",
    "    \n",
    "    if date_col == 'date':\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    if sent_col == 'sentiment':\n",
    "        df[sent_col] = df[sent_col].replace(',','', regex=True)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def insert_tweets(path, dir, file):\n",
    "    \n",
    "    try:\n",
    "        with open(path + dir + '/' + file, 'r') as f:\n",
    "            df = pd.DataFrame(json.load(f))\n",
    "        df = treat_text(df, 'text', date_col = None, sent_col = None)\n",
    "        df_to_postgres(df, 'smi_tweets', conn)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tweets = '../../../../context/SMI/data/get_tweets/2015/DF_2015_12_22.json'\n",
    "with open(path_tweets, 'r') as f:\n",
    "    df = pd.DataFrame(json.load(f))\n",
    "df = treat_text(df, 'text', date_col = None, sent_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carlosriverao</td>\n",
       "      <td>2015-12-22 23:42:00</td>\n",
       "      <td>Google is working on an AI-powered chat servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carlosriverao</td>\n",
       "      <td>2015-12-22 04:20:00</td>\n",
       "      <td>Google and Ford reportedly creating a new comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RitaMRG</td>\n",
       "      <td>2015-12-23 00:47:00</td>\n",
       "      <td>Por que @LauraMCJ3 es tan amor? Es taaaaaan cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RitaMRG</td>\n",
       "      <td>2015-12-22 23:09:00</td>\n",
       "      <td>PATÉTICOS! Sofía o aritz? JA! Gh hasta nunkiii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMBarcelona</td>\n",
       "      <td>2015-12-22 16:48:00</td>\n",
       "      <td>Entre els #ajutsCOMB també trobaràs activitats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267337</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 11:33:00</td>\n",
       "      <td>@chicadeltirso93 @A3Noticias a que te refieres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267338</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 11:10:00</td>\n",
       "      <td>@moedetriana probrabremente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267339</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 10:23:00</td>\n",
       "      <td>@dramatictone solo falta la publicidad del San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267340</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 10:09:00</td>\n",
       "      <td>@perroflautak @dramatictone @_ju1_ las habría ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267341</th>\n",
       "      <td>Pappa_</td>\n",
       "      <td>2015-12-22 02:29:00</td>\n",
       "      <td>El de la familia te amo https://www. instagram...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267342 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             username                 date  \\\n",
       "0       carlosriverao  2015-12-22 23:42:00   \n",
       "1       carlosriverao  2015-12-22 04:20:00   \n",
       "2             RitaMRG  2015-12-23 00:47:00   \n",
       "3             RitaMRG  2015-12-22 23:09:00   \n",
       "4        COMBarcelona  2015-12-22 16:48:00   \n",
       "...               ...                  ...   \n",
       "267337   Mr_Sutilezas  2015-12-22 11:33:00   \n",
       "267338   Mr_Sutilezas  2015-12-22 11:10:00   \n",
       "267339   Mr_Sutilezas  2015-12-22 10:23:00   \n",
       "267340   Mr_Sutilezas  2015-12-22 10:09:00   \n",
       "267341         Pappa_  2015-12-22 02:29:00   \n",
       "\n",
       "                                                     text  \n",
       "0       Google is working on an AI-powered chat servic...  \n",
       "1       Google and Ford reportedly creating a new comp...  \n",
       "2       Por que @LauraMCJ3 es tan amor? Es taaaaaan cu...  \n",
       "3       PATÉTICOS! Sofía o aritz? JA! Gh hasta nunkiii...  \n",
       "4       Entre els #ajutsCOMB també trobaràs activitats...  \n",
       "...                                                   ...  \n",
       "267337  @chicadeltirso93 @A3Noticias a que te refieres...  \n",
       "267338                        @moedetriana probrabremente  \n",
       "267339  @dramatictone solo falta la publicidad del San...  \n",
       "267340  @perroflautak @dramatictone @_ju1_ las habría ...  \n",
       "267341  El de la familia te amo https://www. instagram...  \n",
       "\n",
       "[267342 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'retweets' in (list(df.columns)):\n",
    "    df['retweets'] = 0\n",
    "if not 'favorites' in (list(df.columns)):\n",
    "    df['favorites'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carlosriverao</td>\n",
       "      <td>2015-12-22 23:42:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google is working on an AI-powered chat servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carlosriverao</td>\n",
       "      <td>2015-12-22 04:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google and Ford reportedly creating a new comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RitaMRG</td>\n",
       "      <td>2015-12-23 00:47:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Por que @LauraMCJ3 es tan amor? Es taaaaaan cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RitaMRG</td>\n",
       "      <td>2015-12-22 23:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PATÉTICOS! Sofía o aritz? JA! Gh hasta nunkiii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMBarcelona</td>\n",
       "      <td>2015-12-22 16:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Entre els #ajutsCOMB també trobaràs activitats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267337</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 11:33:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@chicadeltirso93 @A3Noticias a que te refieres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267338</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 11:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@moedetriana probrabremente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267339</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 10:23:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@dramatictone solo falta la publicidad del San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267340</th>\n",
       "      <td>Mr_Sutilezas</td>\n",
       "      <td>2015-12-22 10:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@perroflautak @dramatictone @_ju1_ las habría ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267341</th>\n",
       "      <td>Pappa_</td>\n",
       "      <td>2015-12-22 02:29:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El de la familia te amo https://www. instagram...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267342 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             username                 date  retweets  favorites  \\\n",
       "0       carlosriverao  2015-12-22 23:42:00         0          0   \n",
       "1       carlosriverao  2015-12-22 04:20:00         0          0   \n",
       "2             RitaMRG  2015-12-23 00:47:00         0          0   \n",
       "3             RitaMRG  2015-12-22 23:09:00         0          0   \n",
       "4        COMBarcelona  2015-12-22 16:48:00         0          0   \n",
       "...               ...                  ...       ...        ...   \n",
       "267337   Mr_Sutilezas  2015-12-22 11:33:00         0          0   \n",
       "267338   Mr_Sutilezas  2015-12-22 11:10:00         0          0   \n",
       "267339   Mr_Sutilezas  2015-12-22 10:23:00         0          0   \n",
       "267340   Mr_Sutilezas  2015-12-22 10:09:00         0          0   \n",
       "267341         Pappa_  2015-12-22 02:29:00         0          0   \n",
       "\n",
       "                                                     text  \n",
       "0       Google is working on an AI-powered chat servic...  \n",
       "1       Google and Ford reportedly creating a new comp...  \n",
       "2       Por que @LauraMCJ3 es tan amor? Es taaaaaan cu...  \n",
       "3       PATÉTICOS! Sofía o aritz? JA! Gh hasta nunkiii...  \n",
       "4       Entre els #ajutsCOMB també trobaràs activitats...  \n",
       "...                                                   ...  \n",
       "267337  @chicadeltirso93 @A3Noticias a que te refieres...  \n",
       "267338                        @moedetriana probrabremente  \n",
       "267339  @dramatictone solo falta la publicidad del San...  \n",
       "267340  @perroflautak @dramatictone @_ju1_ las habría ...  \n",
       "267341  El de la familia te amo https://www. instagram...  \n",
       "\n",
       "[267342 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['username', 'date', 'retweets', 'favorites', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "267337    0\n",
       "267338    0\n",
       "267339    0\n",
       "267340    0\n",
       "267341    0\n",
       "Name: retweets, Length: 267342, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['retweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 2015\n",
      "importing DF_2015_11_21.json\n",
      "importing DF_2015_8_21.json\n",
      "importing DF_2015_12_21.json\n",
      "invalid input syntax for type numeric: \"Mobile's Workplace Role Continues to Grow http://www. emarketer.com/Article/Mobile s-Workplace-Role-Continues-Grow/1013365 … http://www. emarketer.com/Article/Mobile s-Workplace-Role-Continues-Grow/1013365 …\"\n",
      "CONTEXT:  COPY smi_tweets, line 1, column retweets: \"Mobile's Workplace Role Continues to Grow http://www. emarketer.com/Article/Mobile s-Workplace-Role-...\"\n",
      "\n",
      "importing DF_2015_7_21.json\n",
      "importing DF_2015_10_21.json\n",
      "importing DF_2015_9_21.json\n",
      "importing DF_2015_1_21.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6541/3176188181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'importing'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minsert_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6541/1208598998.py\u001b[0m in \u001b[0;36minsert_tweets\u001b[0;34m(path, dir, file)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreat_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdf_to_postgres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'smi_tweets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6541/1208598998.py\u001b[0m in \u001b[0;36mdf_to_postgres\u001b[0;34m(df, table, conn)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#Copy cached dataframe into postgres:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_tweets = '../../../../context/SMI/data/get_tweets/'\n",
    "dirs = os.listdir(path_tweets)\n",
    "\n",
    "for dir in dirs:\n",
    "    print(\"Importing\", dir)\n",
    "    files = os.listdir(path_tweets + dir + '/')\n",
    "    for file in files:\n",
    "        print('Importing' , file)\n",
    "        insert_tweets(path_tweets, dir, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
