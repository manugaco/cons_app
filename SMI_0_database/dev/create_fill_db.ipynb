{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import psycopg2\n",
    "import json\n",
    "from io import StringIO\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db config\n",
    "with open('../config/db.config') as config_file:\n",
    "    db_config = json.load(config_file)\n",
    "\n",
    "# Local database deployment\n",
    "conn = psycopg2.connect(\n",
    "                        dbname=db_config['db_name'],\n",
    "                        user=db_config['db_user'],\n",
    "                        host=db_config['db_host'],\n",
    "                        port=db_config['db_port'],\n",
    "                        password=db_config['db_password'],\n",
    "                        options=db_config['db_options']\n",
    "                        )\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "schema = db_config['db_schema']\n",
    "\n",
    "# Paths:\n",
    "queries_path = \"queries/\"\n",
    "temp_data_path = \"../data/\"\n",
    "db_users_bkp = '2018-06-01'\n",
    "db_ini_users_bkp = str(dt.today().strftime(\"%Y-%m-%d\"))\n",
    "db_today = str(dt.today().strftime(\"%Y-%m-%d\"))\n",
    "db_munlist_bkp = 'db_munlist'\n",
    "logs_path = \"logs/\"\n",
    "app_name = \"db_creation\"\n",
    "\n",
    "with open(temp_data_path + db_munlist_bkp + '.json') as config_file:\n",
    "    db_munlist = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories:\n",
    "if not os.path.isdir(logs_path):\n",
    "    print('Environment job: Creating ' + app_name + ' logs directory')\n",
    "    os.makedirs(logs_path)\n",
    "\n",
    "if not os.path.isdir(temp_data_path):\n",
    "    print('Environment job: Creating ' + app_name + ' data directory')\n",
    "    os.makedirs(temp_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment job: Checking folder logs/\n",
      "Environment job: Folder logs/ exists\n",
      "Environment job: Checking folder ../data/\n",
      "Environment job: Folder ../data/ exists\n"
     ]
    }
   ],
   "source": [
    "# Directories check:\n",
    "print('Environment job: Checking folder '+ logs_path)\n",
    "if os.path.isdir(logs_path):\n",
    "    print('Environment job: Folder '+ logs_path + ' exists')\n",
    "else:\n",
    "    print('Environment job: Folder '+ logs_path + ' does not exists')\n",
    "    print('Environment job: Creating folder '+ logs_path)\n",
    "    os.makedirs(logs_path)\n",
    "\n",
    "print('Environment job: Checking folder '+ temp_data_path)\n",
    "if os.path.isdir(temp_data_path):\n",
    "    print('Environment job: Folder '+ temp_data_path + ' exists')\n",
    "else:\n",
    "    print('Environment job: Folder '+ temp_data_path + ' does not exists')\n",
    "    print('Environment job: Creating folder '+ temp_data_path)\n",
    "    os.makedirs(temp_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseCreation:\n",
    "    '''\n",
    "    Database creation and initial data insertion:\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                queries_path,\n",
    "                conn,\n",
    "                schema\n",
    "                ):\n",
    "        self.queries_path = queries_path\n",
    "        self.conn = conn\n",
    "        self.cur = cur\n",
    "        self.schema = schema\n",
    "\n",
    "    @staticmethod\n",
    "    def get_info(url, header):\n",
    "        '''\n",
    "        Function to get the initial twitter users file.\n",
    "        Params: \n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Uncleaned dataframe with parsed tables.\n",
    "        '''\n",
    "        try:\n",
    "\n",
    "            #Get text from url:\n",
    "            page = requests.get(url, headers=header)\n",
    "            soup = BeautifulSoup(page.text, \"lxml\")\n",
    "            results = soup.find(id=\"listado\")\n",
    "\n",
    "            #Get table from text:\n",
    "            df = pd.DataFrame([[tr for tr in tab] for tab in results.table])\n",
    "            df = df.loc[:,1:]\n",
    "\n",
    "            #Get column names:\n",
    "            colnames = [str(name) for name in df.iloc[0]]\n",
    "            colnames = [re.search('<b>(.*)</b>', name).group(1).replace('<br/>', ' ') for name in colnames if name != 'None']\n",
    "            df = df.loc[1:,2:]\n",
    "            df.columns = colnames\n",
    "\n",
    "            #Dropping columns:\n",
    "            df = df[~df['Twittero'].isnull()].reset_index(drop=True)\n",
    "            return(df)\n",
    "\n",
    "        except:\n",
    "            print('Get info from XML files error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_user(input):\n",
    "        '''\n",
    "        Function to extract user name from the list.\n",
    "        Params:\n",
    "            - input: Twitter message.\n",
    "        Output: Twitter user.\n",
    "        '''\n",
    "        try:\n",
    "            #Locating the username:\n",
    "            s = str(input)\n",
    "            start = s.find(\">@\") + len(\">@\")\n",
    "            end = s.find(\"<br/\")\n",
    "            substring = s[start:end]\n",
    "\n",
    "            #Returning the username from:\n",
    "            return(substring)\n",
    "\n",
    "        except:\n",
    "            print('Get username from XML format error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n(input):\n",
    "        '''\n",
    "        Function to extract numeric values from the table:\n",
    "        Params:\n",
    "            - input: Different values from twitter users accounts.\n",
    "        Output: Value\n",
    "        '''\n",
    "        try:\n",
    "            #Locating the value;\n",
    "            s = str(input)\n",
    "            start = s.find(\">\") + len(\">\")\n",
    "            end = s.find(\"</td\")\n",
    "            substring = s[start:end]\n",
    "\n",
    "            #Returning the value:\n",
    "            return(substring.replace(',', ''))\n",
    "\n",
    "        except:\n",
    "            print('Get numeric variables from XML format error')\n",
    "            return 1\n",
    "\n",
    "    def get_initial_users_table(self, urls, headers):\n",
    "        '''\n",
    "        Function to create the initial users dataframe.\n",
    "        Params:\n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Dataframe with cleaned data.\n",
    "        '''\n",
    "        try:\n",
    "            #Scrap info from wp:\n",
    "            info = self.get_info(urls, headers)\n",
    "\n",
    "            #Columns to clean:\n",
    "            cols = ['Twittero', 'Seguido por', 'Sigue a', 'Tweets', 'Twitea desde', 'Ultimo Tweet', 'Categoria']\n",
    "\n",
    "            #Clean columns:\n",
    "            for col in cols:\n",
    "                if col == 'Twittero':\n",
    "                    users = [self.get_user(info[col][i]) for i in range(info.shape[0])]\n",
    "                    info[col] = users\n",
    "                else:\n",
    "                    info[col] = [self.get_n(info[col][i]) for i in range(info.shape[0])]\n",
    "            return(info)\n",
    "\n",
    "        except:\n",
    "            print('Get initial users table')\n",
    "            return 1\n",
    "\n",
    "    def get_tw_users_list(self, urls, headers, ini_users_dict):\n",
    "        '''\n",
    "        Function to get all users from different urls\n",
    "        Params:\n",
    "            - url: list of url pages with the most followed spanish twitter accounts.\n",
    "            - header: specifies robots.txt user agent.\n",
    "        Output: Users list and users table\n",
    "        '''\n",
    "        #Get all users from tables of different sections:\n",
    "        try:\n",
    "            users = []\n",
    "            df_out = pd.DataFrame()\n",
    "            for i in range(len(urls)):\n",
    "                \n",
    "                #Create users dataframe and users list:\n",
    "                df = self.get_initial_users_table(urls[i][0], headers)\n",
    "                df_out = pd.concat([df_out, df], axis=0).reset_index(drop=True)\n",
    "                users.extend(df['Twittero'].to_list())\n",
    "\n",
    "            #Drop duplicates:\n",
    "            if len(users) != len(set(users)):\n",
    "                users = list(set(users))\n",
    "                df_out = df_out.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "            #Formating columns:\n",
    "            df_out.columns = [key for key in list(ini_users_dict.keys())]\n",
    "            df_out = df_out.astype(ini_users_dict)\n",
    "            df_out['lastTweet'] = np.where(df_out['lastTweet']=='n/d', df_out['tweetsSince'], df_out['lastTweet'])\n",
    "            df_out['tweetsSince']=pd.to_datetime(df_out['tweetsSince'])\n",
    "            df_out['lastTweet']=pd.to_datetime(df_out['lastTweet'])\n",
    "            df_out = df_out.fillna(0)\n",
    "            return(df_out)\n",
    "\n",
    "        except:\n",
    "            print('Get twitter users list error')\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_usrs_loc(df, munlist):\n",
    "        '''\n",
    "        Function to filter the location field given a municipalities list, to ensure spanish users:\n",
    "        params:\n",
    "            - df: input dataframe with users information:\n",
    "            - munlist: list of municipalities:\n",
    "        Output: filtered users table.\n",
    "        '''\n",
    "        #Convert location field to lower case:\n",
    "        df['location'] = df['location'].apply(lambda r: r.replace(',', ''))\n",
    "        df['location'] = df['location'].apply(lambda r: r.lower())\n",
    "\n",
    "        #Filter location:\n",
    "        df = df[df['location'].isin(munlist)]\n",
    "        \n",
    "        return(df)\n",
    "\n",
    "    def backup_check(self, path, db_munlist, kind):\n",
    "        '''\n",
    "        Function to check whether there are backups.\n",
    "        params:\n",
    "            - path: relative path to the backup file.\n",
    "            - kind: initial users or users.\n",
    "        Output: \n",
    "            - df: users dataframe.\n",
    "            - usr_ls: list of screenName users.\n",
    "            - check: boolean to check whether there are backup or not.\n",
    "        '''\n",
    "        print('Data job: Check if ' + kind + ' backup exists.')\n",
    "        if os.path.isfile(path):\n",
    "\n",
    "            print('Data job: ' + kind + ' backup exists. Loading file: ' + path)\n",
    "            with open(path, 'r') as f:\n",
    "\n",
    "                df = pd.json_normalize(json.load(f))\n",
    "                if kind == 'users':\n",
    "                    print('Data Engineering job: Filtering location from backup users.')\n",
    "                    print('Data Engineering job: Observations before filter: ' + str(df.shape[0]))\n",
    "                    df = self.filter_usrs_loc(df, db_munlist)\n",
    "                    df['ff_lookup'] = False\n",
    "                    print('Data Engineering job: Observations after filter: ' + str(df.shape[0]))\n",
    "                usr_ls = df['screenName'].to_list()\n",
    "                check = True\n",
    "                print('Data job: ' + kind + ' backup from json file retrieved.')\n",
    "        else:\n",
    "\n",
    "            print('Data job: ' + kind + ' backup does not exists. ')\n",
    "            df = pd.DataFrame()\n",
    "            usr_ls = []\n",
    "            check = False\n",
    "        \n",
    "        return(df, usr_ls, check)\n",
    "\n",
    "    @staticmethod\n",
    "    def fetchone_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to fetch one observation from a query to database:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                return(cur.fetchone()[0])\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error) \n",
    "    \n",
    "    @staticmethod\n",
    "    def fetchall_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to fetch all observations from a query to databasee:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f: \n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                db_fetch = cur.fetchall()\n",
    "                db_fetch = [db_fetch[i][0] for i in range(len(db_fetch))]\n",
    "                return(db_fetch)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "\n",
    "    @staticmethod\n",
    "    def query_SQL(path):\n",
    "        \"\"\"\n",
    "        Function to make a query to database:\n",
    "        params:\n",
    "            - path: relative path to the file.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_postgres(conn, df, table):\n",
    "        \"\"\"\n",
    "        Function to save dataframe into postgres with copy_from:\n",
    "        params:\n",
    "            - conn: database connection.\n",
    "            - df: pandas dataframe.\n",
    "            - table: database table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #Buffering the dataframe into memory:\n",
    "            buffer = StringIO()\n",
    "            df.to_csv(buffer, header=False, index=False)\n",
    "            buffer.seek(0)\n",
    "\n",
    "            #Copy cached dataframe into postgres:\n",
    "            cur = conn.cursor()\n",
    "            cur.copy_from(buffer, table, sep=\",\")\n",
    "            conn.commit()\n",
    "            \n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            conn.rollback()\n",
    "            cur.close()\n",
    "            print(error)\n",
    "            return 1\n",
    "        cur.close()\n",
    "\n",
    "    def db_cs(self):\n",
    "        '''\n",
    "        Function to check and create the database schema and tables.\n",
    "        params: selft referenced, no params.\n",
    "        '''\n",
    "        # Check if the schema exists.\n",
    "        print('Database job: Check if SMI schema exists.')\n",
    "        schema_check = self.fetchone_SQL(queries_path + 'SMI_schema_check.sql')\n",
    "\n",
    "        # If schema exist, check tables.\n",
    "        if schema_check:\n",
    "\n",
    "            # Check initial users table.\n",
    "            print('Database job: SMI schema exists on DB.')\n",
    "            print('Database job: Check if initial users table exist on DB.')\n",
    "            db_ini_check = self.fetchone_SQL(queries_path + 'SMI_ini_users_check.sql')\n",
    "\n",
    "            # If exists, do nothing.\n",
    "            if db_ini_check:\n",
    "                print('Database job: Initial users table exist on DB.')\n",
    "\n",
    "            # If it does not exist, create initial users table.\n",
    "            else:\n",
    "                print('Database job: Initial users table does not exist on DB.')\n",
    "                print('Database job: Creating initial users table on DB.')\n",
    "                self.query_SQL(queries_path + 'SMI_ini_users_table_creation.sql')\n",
    "                print('Database job: Initial users table created on DB.')\n",
    "\n",
    "            # Check users table.\n",
    "            print('Database job: Check if users table exist on DB.')\n",
    "            db_usr_check = self.fetchone_SQL(queries_path + 'SMI_usrs_table_check.sql')\n",
    "\n",
    "            #If exists, do nothing.\n",
    "            if db_usr_check:\n",
    "                print('Database job: Users table exist on DB.')\n",
    "\n",
    "            #If it does not exist, create users table.\n",
    "            else:\n",
    "                print('Database job: Users table does not exist on DB.')\n",
    "                print('Database job: Creating users table on DB.')\n",
    "                self.query_SQL(queries_path + 'SMI_usrs_table_creation.sql')\n",
    "                print('Database job: Users table created on DB.')\n",
    "\n",
    "        # If schema does not exists, create schema and tables.\n",
    "        else:\n",
    "            print('Database job: SMI schema does not exist.')\n",
    "            print('Database job: Cold start - Creating SMI schema and tables on DB.')\n",
    "            self.query_SQL(queries_path + 'SMI_coldstart_database.sql')\n",
    "            print('Database job: DB schema and tables created.')\n",
    "\n",
    "    def scrap_users(self):\n",
    "        '''\n",
    "        Function to scrap initial users from url:\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        # Scrap ini users, create backup and fill database table:\n",
    "        print('Scraping job: Retrieve initial users from url.')\n",
    "        df = self.get_tw_users_list(urls, headers, ini_users_dict)\n",
    "        print('Scraping job: Initial users from url retrieved.')\n",
    "        print('Scraping job: Save initial users to json backup.')\n",
    "        df.to_json(temp_data_path + 'db_ini_users_' + db_today + '.json', orient='records', date_format='iso')\n",
    "        self.df_to_postgres(conn, df, initial_users_table)\n",
    "        print('Database job: Initial users table created on DB.')\n",
    "\n",
    "    def insert_ini_users(self):\n",
    "        '''\n",
    "        Function to insert initial users into DB.\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        db_ini_ls = self.fetchall_SQL(queries_path + 'SMI_ini_database_screenName.sql')\n",
    "        path_ini = temp_data_path + 'db_ini_users_' + db_ini_users_bkp + '.json'\n",
    "        df_ini_users, df_ini_ls, df_ini_user_check = self.backup_check(path_ini, db_munlist, kind = 'initial users')\n",
    "\n",
    "        if df_ini_user_check:\n",
    "            \n",
    "            if (len(db_ini_ls) == 0):\n",
    "                print('Database job: Initial users table is empty.')\n",
    "                print('Database job: Insert initial users backup into DB.')\n",
    "                self.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "            else:\n",
    "                print('Database job: Initial users table is not empty.')\n",
    "                print('Data job: Compare initial users on DB and backup.')\n",
    "\n",
    "                df_ini_ls.sort()\n",
    "                db_ini_ls.sort()\n",
    "                \n",
    "                if df_ini_ls == db_ini_ls:\n",
    "                    print('Data job: initial users match.')\n",
    "                else:\n",
    "                    print('Database job: initial users do not match, drop and create initial users table.')\n",
    "                    #Create initial users table:\n",
    "                    print('Database job: Creating initial users table on DB.')\n",
    "                    self.query_SQL(queries_path + 'SMI_ini_users_table_creation.sql')\n",
    "                    \n",
    "                    #Initial users table insertion:\n",
    "                    print('Database job: Insert initial users back into DB.')\n",
    "                    self.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "                    print('Database job: Initial users table inserted on DB.')\n",
    "        else:\n",
    "            self.scrap_users()\n",
    "\n",
    "    def insert_users(self):\n",
    "        '''\n",
    "        Function to insert users backup into DB.\n",
    "        params: self referenced, no params.\n",
    "        '''\n",
    "        db_usr_ls = self.fetchall_SQL(queries_path + 'SMI_usrs_database_screenName.sql')\n",
    "        path_usr = temp_data_path + 'db_users_' + db_users_bkp + '.json'\n",
    "        df_usr, df_usr_ls, df_usr_check = self.backup_check(path_usr, db_munlist, kind = 'users')\n",
    "\n",
    "        if df_usr_check:\n",
    "            \n",
    "            if (len(db_usr_ls) == 0):\n",
    "                print('Database job: Users table is empty.')\n",
    "                print('Database job: Insert users backup into DB.')\n",
    "                self.df_to_postgres(conn, df_usr, users_table)\n",
    "            else:\n",
    "                print('Database job: Users table is not empty.')\n",
    "                print('Data job: Compare users on db and backup.')\n",
    "\n",
    "                df_usr_ls.sort()\n",
    "                db_usr_ls.sort()\n",
    "                \n",
    "                if df_usr_ls == db_usr_ls:\n",
    "                    print('Data job: Users match.')\n",
    "                else:\n",
    "                    print('Database job: Users do not match, drop and create users table.')\n",
    "                    #Create initial users table:\n",
    "                    print('Database job: Creating users table on DB.')\n",
    "                    self.query_SQL(queries_path + 'SMI_usrs_table_creation.sql')\n",
    "                    \n",
    "                    #Initial users table insertion:\n",
    "                    print('Database job: Users back into DB.')\n",
    "                    self.df_to_postgres(conn, df_usr, users_table)\n",
    "                    print('Database job: Users table inserted on DB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database job: Check if SMI schema exists.\n",
      "Database job: SMI schema exists on DB.\n",
      "Database job: Check if initial users table exist on DB.\n",
      "Database job: Initial users table exist on DB.\n",
      "Database job: Check if users table exist on DB.\n",
      "Database job: Users table exist on DB.\n",
      "Data job: Check if initial users backup exists.\n",
      "Data job: initial users backup exists. Loading file: ../data/db_ini_users_2022-02-08.json\n",
      "Data job: initial users backup from json file retrieved.\n",
      "Database job: Initial users table is not empty.\n",
      "Data job: Compare initial users on DB and backup.\n",
      "Data job: initial users match.\n",
      "Data job: Check if users backup exists.\n",
      "Data job: users backup exists. Loading file: ../data/db_users_2018-06-01.json\n",
      "Data Engineering job: Filtering location from backup users.\n",
      "Data Engineering job: Observations before filter: 673327\n",
      "Data Engineering job: Observations after filter: 240232\n",
      "Data job: users backup from json file retrieved.\n",
      "Database job: Users table is empty.\n",
      "Database job: Insert users backup into DB.\n"
     ]
    }
   ],
   "source": [
    "#Create class instance:\n",
    "dbcreate = DatabaseCreation(queries_path, conn, schema)\n",
    "\n",
    "## Check schema and tables:\n",
    "dbcreate.db_cs()\n",
    "\n",
    "## Check backups, tables and fill database:\n",
    "## Initial users:\n",
    "dbcreate.insert_ini_users()\n",
    "\n",
    "## Users:\n",
    "dbcreate.insert_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job: Retrieve initial users from url.\n",
      "Scraping job: Initial users from url retrieved.\n",
      "Data job: Retrieve users from json file.\n",
      "Data job: Backup users from json file retrieved.\n",
      "Database job: Check if smi schema exists.\n",
      "Database job: smi schema exists.\n",
      "Database job: Check if initial users table exist.\n",
      "Database job: Initial users table exist.\n",
      "Database job: Checking if initial users screenName on DB matches the input.\n",
      "Database job: initial users match.\n",
      "Database job: Check if backup users table exist.\n",
      "Database job: Backup users table exist.\n",
      "Database job: Checking if backup users screenName on DB matches the input.\n",
      "Database job: Backup users do not match, drop and create backup users table.\n",
      "Database job: Creating backup users table on DB.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9771/1015268067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Database job: Creating backup users table on DB.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m#Backup users table creation and insertion:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mdbcreate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_to_postgres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers_table_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Database job: Backup users table created on DB.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "#Initial users:\n",
    "print('Scraping job: Retrieve initial users from url.')\n",
    "df_ini_users = dbcreate.get_tw_users_list(urls, headers, ini_users_dict)\n",
    "df_ini_check = df_ini_users['screenName'].to_list()\n",
    "print('Scraping job: Initial users from url retrieved.')\n",
    "\n",
    "#Users backup:\n",
    "print('Data job: Retrieve users from json file.')\n",
    "if os.path.isdir(temp_data_path):\n",
    "\n",
    "    with open(temp_data_path + 'db_users_' + db_users_bkp + '.json', 'r') as f:\n",
    "        users = json.load(f)\n",
    "\n",
    "    #Backup users list\n",
    "    df_users = pd.json_normalize(users)\n",
    "    df_users['location'] = df_users['location'].apply(lambda r: r.replace(',', ''))\n",
    "    df_user_check = df_users['screenName'].to_list()\n",
    "    print('Data job: Backup users from json file retrieved.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Data job: Backup users from json file does not exists.')\n",
    "    df_users = False\n",
    "\n",
    "#Check schema:\n",
    "print('Database job: Check if smi schema exists.')\n",
    "\n",
    "with open(queries_path + 'SMI_schema_check.sql', 'r') as f:\n",
    "    query = f.read().format(schema=schema)\n",
    "    cur.execute(query)\n",
    "    schema_check = cur.fetchone()[0]\n",
    "\n",
    "if schema_check:\n",
    "\n",
    "    print('Database job: smi schema exists.')\n",
    "    print('Database job: Check if initial users table exist.')\n",
    "\n",
    "    with open(queries_path + 'SMI_ini_users_check.sql', 'r') as f:\n",
    "        query = f.read().format(schema=schema)\n",
    "        cur.execute(query)\n",
    "        db_ini_check = cur.fetchone()[0]\n",
    "\n",
    "    if db_ini_check:\n",
    "\n",
    "        print('Database job: Initial users table exist.')\n",
    "        print('Database job: Checking if initial users screenName on DB matches the input.')\n",
    "        with open(queries_path + 'SMI_ini_database_screenName.sql', 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            cur.execute(query)\n",
    "            db_ini_check = cur.fetchall()\n",
    "            db_ini_check = [db_ini_check[i][0] for i in range(len(db_ini_check))]\n",
    "\n",
    "        df_ini_check.sort()\n",
    "        db_ini_check.sort()\n",
    "\n",
    "        if df_ini_check == db_ini_check:\n",
    "            print('Database job: initial users match.')\n",
    "\n",
    "        else:\n",
    "            print('Database job: initial users do not match, drop and create initial users table.')\n",
    "            #Create initial users table:\n",
    "            with open(queries_path + 'SMI_ini_users_table_creation.sql', 'r') as f:\n",
    "                query = f.read().format(schema=schema)\n",
    "                try:\n",
    "                    cur.execute(query)\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    conn.rollback()\n",
    "                    cur.close()\n",
    "                    print(error)\n",
    "            print('Database job: Creating initial users table on DB.')\n",
    "            #Initial users table creation and insertion:\n",
    "            dbcreate.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "            print('Database job: Initial users table created on DB.')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Database job: Initial users table does not exist.')\n",
    "        #Create initial users table:\n",
    "        with open(queries_path + 'SMI_ini_users_table_creation.sql', 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "        print('Database job: Creating initial users table on DB.')\n",
    "        #Initial users table creation and insertion:\n",
    "        dbcreate.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "        print('Database job: Initial users table created on DB.')\n",
    "\n",
    "    print('Database job: Check if backup users table exist.')\n",
    "    with open(queries_path + 'SMI_usrs_table_check.sql', 'r') as f:\n",
    "        query = f.read().format(schema=schema)\n",
    "        cur.execute(query)\n",
    "        db_usr_check = cur.fetchone()[0]\n",
    "\n",
    "    if db_usr_check:\n",
    "\n",
    "        print('Database job: Backup users table exist.')\n",
    "        print('Database job: Checking if backup users screenName on DB matches the input.')\n",
    "        with open(queries_path + 'SMI_usrs_database_screenName.sql', 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            cur.execute(query)\n",
    "            db_usr_check = cur.fetchall()\n",
    "            db_usr_check = [db_usr_check[i][0] for i in range(len(db_usr_check))]\n",
    "\n",
    "        df_user_check.sort()\n",
    "        db_usr_check.sort()\n",
    "\n",
    "        if df_user_check == db_usr_check:\n",
    "            print('Database job: Backup users match.')\n",
    "\n",
    "        else:\n",
    "            print('Database job: Backup users do not match, drop and create backup users table.')\n",
    "            #Create backup users table:\n",
    "            with open(queries_path + 'SMI_usrs_table_creation.sql', 'r') as f:\n",
    "                query = f.read().format(schema=schema)\n",
    "                try:\n",
    "                    cur.execute(query)\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    conn.rollback()\n",
    "                    cur.close()\n",
    "                    print(error)\n",
    "            print('Database job: Creating backup users table on DB.')\n",
    "            #Backup users table creation and insertion:\n",
    "            if not df_users:\n",
    "                dbcreate.df_to_postgres(conn, df_users, users_table_name)\n",
    "                print('Database job: Backup users table created on DB.')\n",
    "            else:\n",
    "                print('Database job: Backup users table empty, no backup file.')\n",
    "    else:\n",
    "    \n",
    "        print('Database job: Backup users table does not exist.')\n",
    "        #Create backup users table:\n",
    "        with open(queries_path + 'SMI_usrs_table_creation.sql', 'r') as f:\n",
    "            query = f.read().format(schema=schema)\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                conn.rollback()\n",
    "                cur.close()\n",
    "                print(error)\n",
    "        print('Database job: Creating backup users table on DB.')\n",
    "        #Backup users table creation and insertion:\n",
    "        if not df_users:\n",
    "            dbcreate.df_to_postgres(conn, df_users, users_table_name)\n",
    "            print('Database job: Backup users table created on DB.')\n",
    "        else:\n",
    "            print('Database job: Backup users table empty, no backup file.')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Database job: smi schema does not exists.')\n",
    "    print('Database job: Creating db schema and tables.')\n",
    "    with open(queries_path + 'SMI_coldstart_database.sql', 'r') as f:\n",
    "        query = f.read().format(schema=schema)\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            conn.rollback()\n",
    "            cur.close()\n",
    "            print(error)\n",
    "    print('Database job: db schema and tables created.')\n",
    "    print('Database job: Insert initial users information into DB')\n",
    "    dbcreate.df_to_postgres(conn, df_ini_users, initial_users_table)\n",
    "    print('Database job: Initial users information inserted')\n",
    "    print('Database job: Insert backup users information into DB')\n",
    "    \n",
    "    if not df_users:\n",
    "        dbcreate.df_to_postgres(conn, df_users, users_table_name)\n",
    "        print('Database job: Backup users table created on DB.')\n",
    "    else:\n",
    "        print('Database job: Backup users table empty, no backup file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = '../../../../context/SMI/data/train_model/TASScorpus.json'\n",
    "with open(path_corpus, 'r') as f:\n",
    "    df = pd.json_normalize(json.load(f))\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df['content'] = df['content'].replace('\"','', regex=True)\n",
    "    df['content'] = df['content'].replace(',','', regex=True)\n",
    "    df['content'] = df['content'].replace(r'\\\\',' ', regex=True)\n",
    "    df['sentiment'] = df['sentiment'].replace(',','', regex=True)\n",
    "    df['content'] = df['content'].replace(r'\\r+|\\n+|\\t+','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid              object\n",
       "user                 object\n",
       "content              object\n",
       "date         datetime64[ns]\n",
       "lang                 object\n",
       "sentiment            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perfectamente imperfecto. Besos RT @CiindyRomero: @AlejandroSanz no lo veo pero lo comienzo a sentir. Ser√° perfecto! 355 240 275 355 262 277'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['tweetid'] == '172131381843984385'].iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
